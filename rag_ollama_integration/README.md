# RAG + Ollama Integration for Legal Petition System

Clean integration of RAG (Retrieval-Augmented Generation) with Ollama for legal petition generation, based on the [LocalAIAgentWithRAG](https://github.com/techwithtim/LocalAIAgentWithRAG) pattern.

## ğŸ—ï¸ Architecture

```
User Query â†’ Vector Store Search â†’ Context Extraction â†’ Ollama Generation â†’ Response
```

## ğŸ“ Structure

```
rag_ollama_integration/
â”œâ”€â”€ __init__.py              # Package initialization
â”œâ”€â”€ main.py                  # Main RAG agent
â”œâ”€â”€ vector_store.py          # Vector store implementation
â”œâ”€â”€ ollama_client.py         # Ollama API client
â”œâ”€â”€ data_loader.py           # Data loading utilities
â”œâ”€â”€ requirements.txt         # Dependencies
â””â”€â”€ README.md               # This file
```

## ğŸš€ Quick Start

### 1. Install Dependencies
```bash
pip install -r rag_ollama_integration/requirements.txt
```

### 2. Load Your Data
```bash
python -m rag_ollama_integration.data_loader
```

### 3. Run the RAG Agent
```bash
python -m rag_ollama_integration.main
```

## ğŸ”§ Components

### LegalRAGAgent
Main agent that orchestrates the RAG + Ollama workflow.

### VectorStore
Handles document storage, embedding generation, and similarity search.

### OllamaClient
Clean interface to Ollama API with proper error handling.

### LegalDataLoader
Converts existing RAG data to the new vector store format.

## ğŸ’¡ Usage Examples

```python
from rag_ollama_integration import LegalRAGAgent

# Initialize agent
agent = LegalRAGAgent()

# Query the system
result = agent.query("Draft a Supreme Court petition for a dowry case")
print(result['response'])
```

## ğŸ¯ Features

- âœ… Clean separation of concerns
- âœ… Proper error handling
- âœ… Logging and monitoring
- âœ… Easy data loading
- âœ… Scalable architecture
- âœ… Based on proven patterns

## ğŸ”„ Integration with Existing System

This integration works alongside your existing dual RAG system and provides a cleaner interface for Ollama integration.

## ğŸ“Š Performance

- Vector search: ~0.1-0.5s
- Ollama generation: ~2-5s
- Total response time: ~2-6s

## ğŸ› ï¸ Configuration

The system uses your existing:
- 445,938 content documents
- 1,508 structure documents
- `lawgorithm:latest` model 